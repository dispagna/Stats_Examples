---
title: "Bayes R Packages Part 2"
author: "Melissa"
date: "11/5/2020"
output: 
  github_document:
    toc: true
    toc_depth: 2
---

# Introduction

In this post, I'll work through the same example in Part 1 using the _rethinking_ package.  Recall that I'm using the mtcars dataset, and I'm interested in a model with response _mpg_ and predictor _disp_.

# Setup Environment

First some basic R environment setup.

```{r results='hide', message=FALSE, warning=FALSE}
rm(list=ls())

library(tidyverse)
library(rethinking)
library(bayesplot)
library(shinystan)
library(rstan)
library(gridExtra)

knitr::opts_chunk$set(out.width = "50%")
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(message=FALSE)
knitr::opts_chunk$set(warning=FALSE)

options("scipen" = 1, "digits" = 4)

set.seed(123)
```

```{r}
library(datasets)
data(mtcars)
head(mtcars)
```


```{r}
mtcars %>%
  ggplot(aes(x=disp, y=mpg)) +
  geom_point(aes(color=factor(cyl))) 
```



Before I start fitting models, I'll calculate the mean and standard deviation of both _mpg_ and _disp_ since I'll need this information later.

```{r}
mu <- mtcars %>% select(mpg, disp) %>% colMeans()
sigma <- mtcars %>% select(mpg, disp) %>% apply(2,sd)

knitr::kable(cbind(mu, sigma), col.names = c("Mean", "Std Dev"))
```

# Linear Model 

Again I'll start with a linear model even though it clearly isn't going to be a great fit to the data. The _rethinking_ package doesn't have default priors, so I need to explcitly choose them:

\begin{align*}
  mpg \sim N(\mu, \sigma^2) \\
  \mu = a + b*disp \\
  a \sim N(25,10) \\
  b \sim U(-0.1, 0) \\
  \sigma \sim Exp(0.2)
\end{align*}

Note: I defined 

```{r}
# Define model
# Note the sign change for mu and b, this seems to be a quirk
# of map2stan that it didn't like b ~ dunif(-0.1, 0)
f <- alist(
  mpg ~ dnorm(mu, sigma),
  mu <- a - b * (disp - 230.7),
  a ~ dnorm(25, 10),
  b ~ dunif(0, 0.1),
  sigma ~ dexp(0.2)
)
```

```{r results='hide'}
# Fit model
# Note the default number of chains = 1, so I'm explicitly setting to 4 here
mdl1 <- map2stan(f,mtcars, chains=4)
```

## Prior Predictive Distribution

Next, I'll examine the prior predictive distribution to see if the default priors seem reasonable.  

```{r include=FALSE}
# Plot prior predictive distribution
N <- 100

prior_samples <- data.frame(a = rnorm(N, 25, 10),
                            b = runif(N, -0.1, 0))

D <- seq(min(mtcars$disp), max(mtcars$disp), length.out = N)

res <- as.data.frame(apply(prior_samples, 1, function(x) x[1] + x[2] * (D))) %>%
  mutate(disp = D) %>%
  pivot_longer(cols=c(-"disp"), names_to="iter") 

res %>%
  ggplot() +
  geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) +
  labs(x="disp", y="prior predictive mpg")
```

```{r}
# Plot prior predictive distribution
N <- 100

prior_samples <- as.data.frame(extract.prior(mdl1, n=N))

D <- seq(min(mtcars$disp), max(mtcars$disp), length.out = N)

res <- as.data.frame(apply(prior_samples, 1, function(x) x[1] - x[2] * (D))) %>%
  mutate(disp = D) %>%
  pivot_longer(cols=c(-"disp"), names_to="iter") 

res %>%
  ggplot() +
  geom_line(aes(x=disp, y=value, group=iter), alpha=0.2) +
  labs(x="disp", y="prior predictive mpg")
```

The priors look reasonable since I know in the real world _mpg_ must be positive and can't increase as _disp_ increases.

## Diagnostic Plots

### Trace Plots

The _traceplot_ function (equivalent to _mcmc_trac_ in the _bayesplot_ package) plots the MCMC draws.

```{r}
traceplot(mdl1@stanfit)
```

Recall that there are three things I am looking for in the trace plot of each chain:

  1. *Good mixing* -  In other words, the chain is rapidly changing values across the full region versus getting "stuck" near a particular value and slowly changing.
  
  2. *Stationarity* - The mean of the chain is relatively stable.
  
  3. *Convergence* - All of the chains spend most of the time around the same high-probability value.
    
The trace plots above look good.  

### Trace Rank Plot

Another alternative is the _trankplot_ function (equivalent to the _mcmc_rank_overlay_ function in the _bayesplot_ package).  

```{r}
trankplot(mdl1)
```

### Effective Sample Size

The _trankplot_ function conveniently also displays the effective sample size (_n_eff_).  But the _precis_ function is another way to get that information.

```{r}
precis(mdl1)
```

## Posterior Distribution

Since the chains and _n_eff_ look good, I'll examine the posterior distribution next. Again, the _precis_ function gives both the point estimates and credible intervals for _a_, _b_ and _sigma_.

```{r}
precis(mdl1)
```

### Posterior Predictive Distribution

Finally, I'll check the posterior predictive distribution. The _postcheck_ function displays a plot for posterior predictive checking.

```{r}
postcheck(mdl1, window=nrow(mtcars))
```

Under the hood, the _postcheck_ function uses the _sim_ function which draws samples from the posterior predictive distribution. So I can also use the _sim_ function directly to create the same posterior predictive distribution plot as I did with _rstanarm_ previously.  

```{r}
library(forcats)

post <- sim(mdl1) %>%
  apply(2, fivenum) %>%
  t() %>%
  as.data.frame()

dat <- mtcars %>%
  select(c("mpg", "disp")) %>%
  rownames_to_column(var="car")

cbind(dat, post) %>%
  ggplot(aes(x=fct_reorder(car, disp))) +
  geom_boxplot(mapping=aes(ymin=V1, lower=V2, middle=V3, upper=V4, ymax=V5),
               stat="identity",
               outlier.shape = NA) +
  geom_point(mapping=aes(y=mpg), color="red") +
  theme(axis.text.x = element_text(angle = 90))
```

Another useful visualization is the expectation of the posterior predictive distribution (i.e., $\mu$). The _link_ function returns the linear predictor, possibly transformed by the inverse-link function. In this case, the model is a Gaussian likelihood with an identity link function, so the _sim_ and _link_ functions return identical results.

```{r}
newdata <- data.frame(disp=seq(min(mtcars$disp), max(mtcars$disp)))

y_rep <- as.data.frame(t(link(mdl1, data=newdata, n=50))) %>%
  cbind(newdata) %>%
  pivot_longer(cols=starts_with("V"), names_to="grp", values_to="mpg")

y_rep %>%
  ggplot(aes(x=disp, y=mpg)) +
  geom_line(aes(group=grp), alpha=0.2) +
  geom_point(data = mtcars, aes(color=factor(cyl))) 
```

This looks very similar to the results as with the _rstanarm_ package.

# Generalized Additive Model

```{r}
library(splines)

num_knots <- 15
knot_list <- quantile(mtcars$disp, probs=seq(0,1,length.out = num_knots))
B <- bs(mtcars$disp, knots=knot_list[-c(1,num_knots)], intercept=TRUE)

cbind(disp=mtcars$disp,B) %>%
#cbind(year=d2$year, B) %>%
  as.data.frame() %>%
  pivot_longer(-disp, names_to="spline", values_to="val") %>%
  ggplot() +
  geom_line(mapping=aes(x=disp, y=val, color=spline))
```


```{r results='hide'}
f <- alist(
  mpg ~ dnorm(mu, sigma),
  mu <- a - B %*% w,
  a ~ dnorm(25, 10),
  w ~ dnorm(0,5),
  sigma ~ dexp(0.2)
)


mdl2 <- quap(f, data=list(mpg=mtcars$mpg, B=B),
             start=list(w=rep(1, ncol(B)))
            )
```

```{r}
precis(mdl2, depth=2)
```


## Posterior Predictive Distribution

And finally, the posterior predictive distribution:

```{r}
mu <- link(mdl2)
mu_mean <- as.data.frame(apply(mu, 2, mean)) %>%
  mutate(disp=mtcars$disp)
colnames(mu_mean) <- c("mpg_ppd", "disp")

mu_PI <- as.data.frame(t(apply(mu,2,PI,0.89))) %>%
  mutate(disp=mtcars$disp)
colnames(mu_PI) <- c("lwr", "upr", "disp")

ggplot() +
  geom_point(data=mtcars, aes(x=disp, y=mpg, color=factor(cyl))) +
  geom_line(data=mu_mean, aes(x=disp, y=mpg_ppd)) +
  geom_ribbon(data=mu_PI, aes(x=disp, ymin=lwr, ymax=upr), alpha=0.2)

```

